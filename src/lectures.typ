#import "/utils.typ": *

#set page("a5", numbering: "1")
#set text(lang: "ru")
#set par(justify: true)
#set heading(numbering: "1.")


`24 Сентября 2024`
= Вводная

Колобов Александр Георгиевич, D947

Чтобы получить экзамен нужно:
- допуск от преподавателя по практике
- сдача теории (экзамен)  

Можно сдавать частями (три коллоквиума):
- Прямые методы
- Итерационные методы
- Собственные значения

Курсовой проект. Подробно разобрать конкретный метод, который каждому будет дан. Ещё будет две теоретические задачи.

#linebreak()

= Первая лекция

В конечномерном пространстве все нормы эквивалентны. 

#def[
  $norm(x)$ --- число, для которого выполняются 3 аксиомы:\
  1. $norm(x) >= 0, norm(x) = 0 <=> x = 0$
  2. $norm(lambda x) = lambda norm(x)$
  3. $norm(x+y) <= norm(x) + norm(y)$
]

#def[
  Нормы вектора.
  $
    &norm(x)_1 = sum_(i=1)^n abs(x_i)\
    &norm(x)_2 = sqrt((x, x)) - "евклидова норма"\
    &norm(x)_oo = max_i abs(x_i)
  $
]

СЛАУ:
$ A x = f $

#def[
  Нормы матрицы.

  Евклидова норма:

  $ norm(A)_E = sqrt(sum_(i, j) a^2_(i j)) $
]

#def[
  Подчинённая матричная норма:
  $ norm(А) = sup_(x!=0) norm(A x) / norm(x) $

  Свойства:
1. $norm(I) = 1 $
2. $norm(A B) <= norm(A) norm(B) $
]



Евклидова норма не является подчинённой, ведь
$norm(I)_E = sqrt(n) != 1$.

Из определения подчинённой матричной нормы вытекает:

$ norm(А) = sup_(x!=0) norm(A x) / norm(x) => norm(A x) <= norm(A) norm(x) $


#def[
  Согласованная матричная норма такая, что:
  $ norm(A x) <= norm(A) norm(x) $
]

$
  &norm(A)_1 = max_j sum_(i=1)^n abs(a_(i j)) \
  &norm(A)_2 = sqrt(max_i lambda_i (A^* A)), " где" A^* - "транспонированная матрица"\
  &norm(A)_oo = max_i sum_(j=1)^n abs(a_(i j))
$

$
  norm(A) = sup_(x!=0) norm(A x) / norm(x)
$

// $ norm(A x)_1 = max_i abs(sum_(j=1)^n a_(i j)x_j)
// <= max_i sum_(j=1)^n abs(a_(i j)) abs(x_j) $

$ norm(A x)_1 =
  sum_(i=1)^n abs(sum_(j=1)^n a_(i j)x_j) <=
  sum_(i=1)^n sum_(j=1)^n abs(a_(i j)) abs(x_j) = \
  = sum_(j=1)^n sum_(i=1)^n abs(a_(i j)) abs(x_j) =
  sum_(j=1)^n (abs(x_j) sum_(i=1)^n abs(a_(i j))) <= \
  <= sum_(j=1)^n ((max_j sum_(i=1)^n abs(a_(i j)))abs(x_j))  =
  max_j sum_i abs(a_(i j)) dot sum_j abs(x_j) =\
  = (max_j sum_i abs(a_(i j))) norm(x)_1
  
$

Доказали, что

$ norm(A x)_1 / norm(x)_1 <= max_j sum_(i=1)^n abs(a_(i j)) $

$k -$  номер столбца, где достигается максимум:

$ sum_(i=1)^n abs(a_(i k)) = max_j sum_(i=1)^n abs(a_(i j)) $

$ e_k = vec(0, dots, underbrace(1, k-"й индекс"), dots, 0, 0) 
  = max_j sum_(i=1)^n abs(a_(i j)) norm(x) ???
$

$ norm(A e_k)_1 = sum_i abs(a_(i k)) $

Для второй матричной нормы  $norm(A)_2 = sqrt(max_i lambda_i (A^* A))$:

$ lambda_i$ должны быть действительными и положительными

$A^* A - $ неотрицательно определена и симметрична

$ (A^* A)^* = A^* (A^*)^* = A^* A $

#def[
  Матрица $C$ называется неотрицательно определённой, если
  $ forall (y, y) >= 0 $
  Похожим образом определяются положительно определённая матрица и другие.
]

Свойство скалярного произведения:

$ (B x, y) = (x, B^* y) $

$ (A^*A x, x) = (A x, A x)) >= 0 $

Из курса линейной алгебры известно, что все собственные значения симметрической матрицы вещественны.

Значит, $norm(A)_2 = sqrt(max_i lambda_i (A^* A))$ имеет смысл для симметрических матриц.

У симметрических матриц существует полная система ортонормированных собственных векторов.

$ (A^* A) u^((i)) = lambda_i u^((i))\
(u^((i)), u^((j))) = cases(
  1 when i = j,
  0 when i != j
)
$

$ x = sum_(i=1)^n alpha_i u^((i)) $

$ norm(x)_2 = sqrt((x, x)) = sqrt((sum_(i=1)^n alpha_i u^((i)), sum_(j=1)^n alpha_j u^((j)))) = \
= sqrt(sum_(i=1)^n alpha_i^2)
$

$
  norm(A x)_2 = sqrt((A x, A x)) =
  sqrt((A^* A x, x)) =\
  = sqrt((A^*A sum_i alpha_i u^((i)), sum_j alpha_j u^((j)) ))
  = sqrt((sum_i alpha_i A^* A u^((i)) comma sum_j alpha_j u^((j)))) =\
  = sqrt((sum_i alpha_i lambda_i u^((i)) comma sum_j alpha_j u^((j))))    
  = sqrt(sum_i alpha_i^2 lambda_i) <=\
  <= sqrt(sum_i alpha_i^2 max_j lambda_i (A^* A))
  = sqrt(max_i lambda_i (A^*A)) norm(x)_2
$

$ lambda_k (A^* A) = max_i lambda_i (A^* A) $

Посчитаем вторую норму. Предположим, что матрица симметричная:
$ A^* = A $

$ norm(A)_2 = sqrt(max_i lambda_i (A^* A)) =
  sqrt(max_i lambda_i (A^2)) =
  sqrt(max_i lambda_i^2 (A)) = 
  max_i abs(lambda_i (A))
$

#linebreak()

`1 Октября 2024`
= Обусловленность матрицы систем

#def[
  Число обусловленности:
$ mu(A) = sup_(x, y != 0)(norm(A x)/norm(x) div norm(A y)/norm(y)) $
]

$ A x = f $

$ A(x+ xi) = f + phi $

$ A xi = phi $

Попробуем оценить следующую величину:

$ norm(xi) / norm(x) dot norm(f) / norm(phi) = 
  norm(xi) / norm(x) dot norm(A x) / norm( A xi) = 
  norm(A x) / norm(x) div norm(A xi) / norm(xi) <=
  mu(A)
$

$ norm(xi) / norm(x) <= mu(a) norm(phi) / norm(f) $

Представим, что $mu(A)$ невелико, меньше единицы оно быть не может, тогда маленькое возмущение правой части гарантирует, что возмущение в решении тоже невелико.

Как влияет погрешность матрицы на результат:

$ (A + Sigma)(x + xi) = f + phi $

$ norm(xi)/norm(x) <= mu(A) / (1 - mu(A) norm(Sigma)/norm(A)) (norm(phi)/norm(f) + norm(Sigma)/norm(A)) $

При условии:

$ norm(A^(-1)) norm(Sigma) < 1 $

Нам нужна формула, по которой мы будем искать число обусловленности, выведем её.

$ mu(A) = sup_(x!=0)(norm(A x)/norm(x)) / (inf_(y!=0)(norm(A y)/norm(y))) 
  = norm(A) / (inf_(z!=0) 1/(norm(A^(-1) z) / norm(z))) =\
  =norm(A) / (1/(sup_(z!=0) norm(A^(-1) z) / norm(z)))
  = norm(A) norm(A^(-1))
$

$ z := A y $
#showybox(breakable: true, title: "Пример")[
  $ A = mat(1, 1.0001; 1.0001, 1) wide f = vec(2.0001, 2.0001) $

  $ x = vec(1, 1) $

  $ norm(A) = max_(i = 1,2) abs(lambda_i (A)) $

  $ matdet(1-lambda, 1.0001; 1.0001, 1-lambda) = 0 $

  $ (1-lambda)^2 - 1.0001^2 = 0 $

  $ lambda_1 = 0.0001 wide lambda_2 = -10^(-4)  $

  $ norm(A) = 2.0001 wide norm(A^(-1)) = 10^4 $

  $ mu(A) = 2001 "большое число" $

  Рассмотрим следующую систему:

  $ (A+Sigma)y = f + phi $
  
  $ Sigma = mat(0, 0; 0, 0.00005) wide
    phi = vec(0.0001, 0)  
  $
  
  Решение системы:
  
  $
    y = vec(0, 2)
  $
]

#linebreak()
= Прямые методы

== Метод Гаусса (метод исключения неизвестных)

Метод в основе которого лежит использование элементарных преобразований матрицы с целью применения её к треугольному или диагональному виду.

$ A x = f $

$ cases(
    a_(11)x_1 + ... + a_(1 n)x_n = f_1,
    dots,
    a_(n 1)x_1 + ... + a_(n n)x_n = f_n,
  )
$

Исключаем элемент $x_1$. Пусть $a_11 != 0$:

$ x_1 + a_(12) / a_11 x_2  + ... + a_(1 n)/a_(11) x_n = f_1 / a_11 | dot a_21 $

$ a_21 x_1 + a_21 a_(12) / a_11 x_2  + ... + a_21 a_(1 n)/a_(11) x_n = a_21 f_1 / a_11 $

Теперь из второго уравнения системы вычитаем вот это, таким образом получим уравнение, в котором нет $x_1$.

$ cases(
    a_(11)x_1 + a_(22) x_2 +... + a_(1 n)x_n = f_1,
    #hide[$a_(11)x_1 + $] a_22^((1))x_2 + ... + a_(2 n)^((1))x_n = f_2^((1)),
    dots,
    a_(n 1)x_1 + a_(n 2)x_2 +... + a_(n n)x_n = f_n,
  )
$

$ cases(
    a_(11)x_1 + a_(22) x_2 +... + a_(1 n)x_n = f_1,
    #hide[$a_(11)x_1 + $] a_22^((1))x_2 + ... + a_(2 n)^((1))x_n = f_2^((1)),
    dots,
    #hide[$a_(11)x_1 + $] a_(n 2)^((1))x_2 +... + a_(n n)^((1))x_n = f_n^((1)),
  )
$

Второй шаг. $a_22^((1)) != 0$ и делаем аналогично.

После $n-1$ шагов получим матрицу, которая имеет верхний треугольный вид.

Теперь запишем формулы всех этих шагов.

1 шаг:

$ d_(i 1) = a_(i 1) / a_11 $

$ a_(i j)^((1)) = a_(i j) - d_(i 1) a_(1 j), quad i, j = 2, ..., n $

$ f_i^((1)) = f_i - d_(i 1) f_1 $

2 шаг:

$ d_(i 2) = a_(i 2)^((1)) / a_22^((1)) $

$ a_(i j)^((2)) = a_(i j)^((1)) - d_(i 2) a_(2 j)^((1)), quad i, j = 3, ..., n $

$ f_i^((2)) = f_i^((1)) - d_(i 2) f_2^((1)) $

$ A_(n-1) x = f^((n -1)) $
$ A_(n-1) =: U "верхняя треугольная матрица" $

$ f^((n-1)) =: g $

$ U x = g $

$ u_(n n) x_n = g_n $

$ x_n = g_n / u_(n n) $

$k-й$ шаг обратного хода:

$ u_(k k) x_k + sum_(j=k+1)^n u_(k j) x_j = g_k $

$ x_k = (g_k - sum_(j=k+1)^n u_(k j)x_j) / u_(k k) $

Условие, при котором ведущий элемент не равен нулю. Во-первых матрица должна быть невырожденной.

#bbox[Критерий работы метода Гаусса][
Отличие от нуля всех главных миноров.]

$ det mat(
  a_11, a_12, ..., a_(1 k);
  a_21, a_22, ..., a_(2 k);
  ..., ..., ..., ... ;
  a_(k 1), a_(k 2), ..., a_(k k)
) != 0 $

$ a_11 != 0 $

$ matdet(a_11, a_12; 0, a_22^((1))) != 0 $

$ a_11 a_22^((1)) != 0 $

$ a_22^((1)) != 0 $

$ matdet(
  a_11, a_12, a_13;
  0, a_22^((1)), a_23^((1));
  0, 0, a_33^((2))
) != 0 $

$ a_11 a_22^((1)) a_33^((2)) != 0 $

$ a_33^((2)) != 0 $

Все главные элементы отличны от нуля.

== LU-разложене

Тот же метод Гаусса, но записанный через разложение матриц.

$ D_1 := mat(
  1, 0, 0;
  -d_21, 1, 0;
  -d_31, 0, 1;
  dots.v;
  -d_(n 1), 0, 0, dots, 1
) $

$ A_1 = D_1 A $

$ D_2 := mat(
  1;
  0, 1, ..., 0;
  0, -d_32, ...;
  0, -d_42, ...;
  dots.v;
  0, -d_(n 2), ..., 1;
  
) $

$ A_2 = D_2 A_1 $

$ A_(n -1) $ 

$ U = A_(n - 1) = D_(n-1) A_(n-2) = D_(n-1) D_(n-2) A_(n-3) = ... $

$ U = D_(n-1) D_(n-2) dots D_1 A $

$ D := D_(n-1) D_(n-2) dots D_1 $

$ U = D A $

$ L := D^((-1)) $

$ A = L U $

Как выглядит эта матрица:

$ L = mat(
  1, ; 
  d_21, 1;
  d_31, d_32, 1;
  dots.v, dots.v, dots.v, dots.down;
  d_(n 1), d_(n 2), d_(n 3), dots, 1
) $

$ L U x = f $

$ U x := y $

$ L y = f $

Докажем необходимость в критерии работы метода Гаусса.

$ A = L U $

$ A^((k)) = L^((k)) U^((k)) $

Подматрица $k$-го порядка.

$ det A^((k)) = det L^((k)) det U^((k)) $

$ det L^((k)) = 1 $

$ det U^((k)) = u_11 u_12 dots u_(k k) $

$ u_(i i) != 0, quad i=overline(1 comma k) $

$ det A^((k)) != 0 $

`08 Октября 2024`
== Метод квадратного корня

Считаем, что все главные миноры не равны нулю.

$ A = L U $
$ U = D V $

$ D = mat(
  d_1;
  ,d_2;
  ,,dots.down;
  ,,,d_n;
) wide  
 V = mat(
  1, v_12, v_13, dots, v_(1 n);
  ,1,v_23,dots, v_(2 n);
  ,,,,dots.v;
  ,,,,1
) $


$ U = mat(
  d_1, d_1 v_12, d_1 v_13, dots, d_1 v_(1 n);
  ,    d_2,      d_2 v_23, dots, d_2 v_(2 n);
  ,            ,         , ,dots.v;
  ,,,,d_n
) $

$ d_i =u_(i i) quad v_(i j) = u_(i j)/(u_(i i)) $

Есть далее понятие LDV-разложение, оно определяется единственным образом.

Пусть $A$ - симметрическая:


$ A = A^* $

$ A = L D V $

$ A^* = (L D V)^* = V^* D^* L^* = (V^* D L^*) = A $

$ => L = V^*, V = L^* $

$ A = V^* D V $

Пусть матрица $A$ положительно определённая. Тогда по критерию Сильвестра все её угловые миноры положительны.

$ (A x, x) > 0 $

$ (D x, x) = (D V y, V y) = (A y, y) > 0 $

$ x = V y $


Все элементы на диагонали в $D$ больше нуля.

$ D^(1/2) = "diag"(sqrt(d_1), ..., sqrt(d_n)) $

$ A = V^* D^(1/2) D^(1/2) V $

$ W = D^(1/2) V $

$ W^* = V^* D^(1/2) $

$ A = W^* W $

$ A x = f $

$ W^* W x = f $

$ y:= W x $

$ W^* y = f $

$ a_(i j) = sum_(k = 1)^n W^*_(i k) W_(k j) = sum_(k = 1)^n W_(k i) W_(k j) = sum_(k = 1)^i W_(k i) W_(k j) $

$ a_11 = W_11 W_11 => W_11 = sqrt(a_11) $

$ i = 1 quad j = 2, 3, ..., n:\ 
  a_(1 j) = W_(1 1)W_(1 j) \
  W_(1 j) = a_(1 j) / W_11
$

$ a_(i i) = sum_(k=1)^i W_(k i) W_(k i) = sum_(k=1)^(i-1) W^2_(k i) + W^2_(i i) $

$ W_(i i) = sqrt(a_(i i) - sum_(k=1)^(i-1) W_(k i)^2) $

$ a_(i j) = sum_(k=1)^(i-1) W_(k i) W_(k j) + W_(i i) W_(i j) $

$ W_(i j) = (a_(i j) - sum_(k=1)^(i-1) W_(k i) W_(k j))/ W_(i i) $

$ j = i+1, ..., n $

Это был метод квадратного корня.

Если матрица симметрическая, но не является положительно определённой, значит решением будут комплексные числа, ничего страшного).

== QR разложение (Пуэр разложение)

#def[
Гиперплоскость с нормалью $p = vec(p_1, dots.v, p_n)$, проходящая через начало координат
$ (p, x) = p_1 x_1 + p_2 x_2 + dots + p_n x_n = 0 $
]

Ортогональное преобразование отражения относительно гиперплоскости, проходящей через начало координат:

$ y = x - 2 ((p, x))/((p, p)) p $


$ y = x - 2 ((p, x))/((p, p)) p = x - 2/((p, p)) (p,x) p = x - 2/((p, p)) p p^* x =\
  = (I - 2/((p,p)) p p^* )x 
$

$ P := I - 2/((p,p)) p p^* $

$ y = P x $

*Свойства:*
+ $P^2 = I$

  $ P^2 x = P(P x) = P( x - 2((p,x))/((p,p)) p) = P x - 2((p,x))/((p,p))P p = \
    = P x - 2 ((p,x))/((p,p)) (p - 2 ((p,p))/((p,p)) p)
    = x - cancel(2 ((p, x))/((p, p))) + cancel(2 ((p,x))/(p, p)) = x  $

+ $P^* =P$
    
  $ P^* = ( I - 2/((p,p)) p p^*)^* = I - 2/((p, p)) p^(**) p^* = p $

+ Матрица ортогональная $P^* P = P^2 = I$

+ Преобразование отражения не изменится, если вместо вектора нормали $p$ взять вектор $beta p$

  $ P = I - 2/((beta p, beta p)) (beta p) (beta p)^* = I - 2/((p, p)) p ^* $

+ Вектор нормали $p$ можно определить как разность между исходным и отражённым вектором: $p = x - y$

  $ x - y = 2 ((p,x))/((p,p)) p $

+ $p_1 = p_2 = dots = p_k = 0$

  $=> y_1 = x_1, y_2 = x_2, dots, y_k = x_k$

  Докажем:
  
  $ y_i = x_i - 2p_i / (sum_(l=1)^n p_l^2) sum_(l=1)^n p_l x_l $

  Из этой формулы очевидно (я хз??? он так сказал на лекции)

+ $p_1 = p_2 = dots = p_k = 0 " и " x_(k+1) = dots = x_n = 0 $

  $=> y_(k+1) = dots = y_n = 0$

$ A = Q R,\ "где" Q "ортогональная матрица, а" R "верхняя треугольная" $

Получим матрицу $A_1$ следующим образом:

$ A_1 = P_1 A $

Как должен выглядеть первый столбец:

$ vec(a_(11)^((1)), 0, 0, dots.v, 0) $

Введём обозначение столбцов матрицы $A$ и $A_1$:
$ a_(j), a^((1))_j$.

$ 
  vec(a_11, a_21, dots.v, a_(n 1)) = a_1
  quad
  vec(a_(11)^((1)), 0, dots.v, 0) = a_1^((1)) 
$

$ P_1 a_1 = vec( a_11^((1)), 0, 0, dots.v, 0) $

Вектор нормали $p^((1)) = a_1 - a_1^((1))$ (разность между исходным и отражённым)

$ &p^((1))_1 = a_11 - a_11^((1)) \
  &p^((1))_2 = a_21 \
  &dots.v \
  &p^((1))_n = a_(n 1)
$

При ортогональном преобразовании длина вектора не меняется:

$ norm(a^((1))_1)_2^2 = norm(a_1)_2^2 $

$ (a_11^((1)))^2  = sum_(l=1)^n a_(l 1)^2 $

$ a_11^((1)) = plus.minus sqrt(sum_(l=1)^n a_(l 1)^2) $

$ p_1^((1)) = a_11 plus.minus sqrt(sum_(l=1)^n a_(l 1)^2) $

$ sigma_1 := cases(
  1 when a_11 >= 0,
  -1 when a_11 < 0
) $

$ p_1^((1)) = a_11 + sigma_1 sqrt(sum_(l=1)^n a_(l i)^2) $

$ a_j^((1)) = a_j - 2 ((p^((1)), a_j)) / ((p_1^((1)), p_1^((1)))) p^((1)) where j = 2, ..., n $

По этой формуле полностью будет определена матрица $A_1$.

`15 Октября 2024`

$ mat(
  a_11^((1)), a_12^((1)), dots, a_(1,k-1)^((1)), a_(1 k)^((1)), a_(1,k+1)^((1)), dots, a_(1 n)^((1));
  ,a_22^((2)), dots, a_(2,k-1)^((2)), a_(2 k)^((1)), a_(2, k+1)^((1)), dots, a_(2 n)^((1));
  ,,dots.down;
  ,,,a_(k-1,k-1)^((k-1)), a_(k-1,k)^((k-1)), a_(k-1, k+1)^((k-1)), dots, a_(k-1, n)^((k-1));
  ,,,,a_(k k)^((k)), a_(k,k+1)^((k)), dots, d_(k,n)^((k));
  ,,,,a_(k+1, k)^((k+1)), a_(k+1,k+1)^((k+1)), dots, d_(k+1,n)^((k+1));
  ,,,,a_(n-1, k)^((n-1)), a_(n-1,k+1)^((n-1)), dots, d_(n-1,n)^((n-1));
) $

$ A_k = P_k A_(k - 1) $

$ a_k^((k)) = P_k a_k^((k-1)) = mat(
  a_(1 k)^((1));
  a_(2 k)^((2));
  dots.v;
  a_(n-1,k)^((k-1));
  a_(k k)^((k));
  0;
  dots.v;
  0
) $

$ p^((k)) = a_k^((k-1)) - a_k^((k)) $

$ p_l^((k)) = 0, quad l = 1, 2, dots, n-1 $

$ p_k^((k)) = a_(k k)^((k-1)) - a_(k k)^((k)) $

$ p_l^((k)) = a_(l k)^((k-1)), quad l = k+1, dots, n $

$ norm(a_k^((k+1)))_2^2 = norm(a_k^((k)))_2^2 $

$ a_(k k)^((k)) = plus.minus sqrt(sum_(l=k)^n (a_(l k)^((k-1)))^2)  $

$ sigma_k = cases(
  1 comma space a_(k k)^((k-1)) >= 0,
  -1 comma space a_(k k)^((k-1)) < 0
) $

$ p_k^((k)) = a_(k k)^((k-1)) + sigma_k sqrt(sum_(l=k)^n (a_(l k)^((k-1)))^2) $

$ a_j^((k)) = a_j^((k-1)) - 2 (p^((k)), a_j^((k-1))) / ((p^((k)), p^((k)))) p^k $

$ a_(i j)^((k)) = a_(i j)^((k-1)) - 2 p_i^((k)) / (sum_(l=n)^n (p_l^((k)))^2) sum_(l=k)^n p_l^((k)) a_(l j)^((k-1))  $

$ j = k+1, dots, n $

$ p_1^((k)) = p_2^((k)) = dots = p_(k-1)^((k)) = 0 $

Нужно сделать $n-1$ шаг. Рассмотрим весь процесс в целом.

$ R = A_(n-1) = P_(n-1) A_(n-2) = P_(n-1) P_(n-2) A_(n-3) = dots =\
  = P_(n-1) P_(n-2) dots P_2 P_1 A
$

$ Q := P_1 P_2 dots P_(n-1) $

Матрица $Q$ ортогональная (как произведение ортогональных матриц).

$ Q^* = P_(n-1)^* dots P_1^* = P_(n-1) dots P_1 $

$ R = Q^* A $

$ A = Q R $

Требования для существования этого разложения:

Рассмотрим случай, когда мы делим на ноль.

$ sum_(l=k)^n (p_l^((k)))^2 = 0  <=> p_l^((k)) = 0, space l = k, dots, n $

Подберём вектор нормали, который будет ненулевым, но оставит вектор на месте. Чтобы формально в алгоритме шаг выполнился. 

Например, $ p^((k)) = vec(0, dots.v, 0, sqrt(2), 0, dots.v, 0) $

Теперь можем говорить, что QR разложение существует для любых матриц.

// $ a_(l k)^((k-1)) = 0, quad l = k, ..., n $

Рассмотрим вопрос решения уравнений с помощью QR разложений.

$ A x = f $
$ Q R x = f quad $
$ R x = Q^* f $

Распишем $Q^*$:

$ Q^* = P_(n-1) dots P_1 $

$ Q^* f := g $

$ f^((1)) = P_1 f $
$ f^((2)) = P_2 f^((1)) $
$ g= f^((n-1)) = P_(n-1) f^((n-2)) $

$ f^((1)) = f - 2 ((p^((1)), f)) / ((p^((1)), p^((1)))) p^((1)) $

$ f^((k)) = f^((k-1)) - 2 p^((k)) / (sum_(l=k)^(n) (p_l^((k)))^2 ) sum_(l = k)^n p_l^((k)) f_l^((k-1)), quad i = k, k+1, dots, n $

== Метод окаймления
Способ отыскания обратной матрицы.

$ A = A_n = mat(augment: #(hline:-1, vline:-1),
  a_11, a_12, dots, a_(1 n-1), a_(1 n);
  a_21, a_22, dots, a_(2 n-1), a_(2 n);
  dots.v;
  a_(n-1, 1), a_(n-1, 2), dots, a_(n-1, n-1), a_(n-1, n);
  a_(n,1), a_(n,2), dots, a_(n, n-1), a_(n, n)
) $

$ A_(n - 1) $

$ v_n = (a_(n 1), a_(n 2), dots, a_(n,n-1)) $

$ u_n = (a_(1 n), a_(2,n), dots, a_(n-1, n))^* $

$ A_n = mat(A_n-1, u_n; v_n, a_(n n)) $

$ A_n^(-1) = mat(P_(n-1), r_n; q_n, 1/alpha_n) = D_n $

Предполагаем, что $A_(n-1)^(-1)$ известна 

$ mat(A_n-1, u_n; v_n, a_(n n)) mat(P_(n-1), r_n; q_n, 1/alpha_n) = mat(E, 0; 0, 1) $

$ cases(
 A_(n-1) P_(n-1) + u_n q_n = E,

 v_n P_(n-1) + a_(n n) q_n = 0,

 A_(n-1) r_n + u_n/alpha_n = 0,

 v_n r_n + a_(n n)/alpha_n = 1 
) $

Возмьём третье и выразим:
#cbox[
$ r_n =  -A_(n-1)^(-1) u_n / alpha_n $]

Подставляем в четвёртое:

$ - (v_n A_(n-1)^(-1) u_n) / alpha_n + a_(n n) / alpha_n  = 1 $

#cbox[
$ alpha_n = a_(n n) - v_n A_(n-1)^(-1) u_n $]

// Перейдём ко второму:

#cbox[
$ P_(n-1) = A_(n-1)^(-1) - A_(n-1)^(-1) u_n q_n $]

$ v_n (A_(n-1)^(-1) - A_(n-1)^(-1) u_n q_n) + a_(n n)q_n = 0 $

$ v_n A_(n-1)^(-1) + (a_(n n)- v_n A_(n-1)^(-1) u_n) q_n = 0 $

$ v_n A_(n-1)^(-1) + alpha_n q_n = 0 $

#cbox[
$ q_n = - (v_n A_(n-1)^(-1))/alpha_n $]

+ $ -A_(n-1)^(-1) u_n wide (beta_(1 n), dots, beta_(n-1, n))^* $
+ $ -v_n A_(n-1)^(-1) wide (gamma_(n 1), dots, gamma_(n, n-1)) $
+ $ alpha_n = a_(n n) + sum_(i=1)^(n-1) a_(n i) beta_(i n) = a_(n n) + sum_(i=1)^(n-1) a_(i n) gamma_(n i) $
+ $ d_(i k) = d_(i k) + (beta_(i k) gamma_(n k))/ alpha_n, quad i, k <= n-1 $
  $ d_(i n) = beta_(i n) / alpha_n quad d_(n k) = gamma_(n k) / alpha_n $

Работа в первом блоке завершена. Можно сдавать первый коллоквиум по прямым методам. Д947.

`22 Октября 2024`
= Итерационные методы

$ A x = f $

$ { x^((k)) } $

$ lim_(k->oo) x^((k)) = x^* $

$ lim_(k->oo) norm(x^((k)) - x^*) = 0 $

Каноническая форма записи:

$ B (x^((k+1)) - x^((k)))/(tau_k) + A x^((k)) = f, space k = 0, 1, dots $

$x^((0)) "начальное приближение"$

$B - "невырожденная матрица"\ tau - "итерационный параметр (может меняться на разных шагах)" $
Если $tau$ не зависит от $k$, то метод называется стационарным.

Если методы сходятся, то они сходятся к точному решению. Поэтому нужно доказывать только сходимость.

Скорость сходимости. Надо добиться наивысшей скорости сходимости. Зависит от параметра и от матрицы $B$.

Сейчас нас будут интересовать только стационарные методы:

$ B (x^((k+1)) - x^((k)))/(tau) + A x^((k)) = f, space k = 0, 1, dots $

$ B x^((k+1)) = B x^((k)) - tau A x^((k)) + tau f $

$ B x^((k+1)) = (B - tau A)x^((k)) + tau f $

Матрица $B$ должна быть простая, чтобы систему можно было просто решать, а то игра не стоит свеч.

Показатели:

+ Вектор погрешности или вектор ошибки $z^((k)) = x^((k)) - x^*$.
+ Вектор невязки: $r^((k)) = A x^((k)) - f$

$ lim_(k->oo) norm(z^((k))) = 0 - "сходимость к точному решению" $

Добавим и отнимем точное решение:

$ B (x^((k+1)) -x^* - x^((k)) + x^*)/tau  + A x^((k)) - A x^* = 0 $

$ B (z^((k+1)) - z^(k)) / tau + A z^((k)) = 0 $

$ B z^((k+1)) = B z^((k)) - tau A z^((k)) $

$ z^((k+1)) = (I - tau B^(-1)A) z^((k)) $

Матрица перехода $S = (I - tau B^(-1)A)$:

$ z^((k+1)) = S z^((k)) $

Для того, чтобы можно было сравнивать скорости сходимости для разных методом существует асимптотическая скорость сходимости.

$ norm(z^((k))) <= 1/e norm(z^((0))) $

Сколько шагов нужно сделать, чтобы начальная ошибка уменьшилась в $e$ раз.

$ z^((k+1)) = S z^((k)) => z^((k)) = S^k z^((0)) $

$ norm(z^((k))) <= norm(S)^k norm(z^((0))) $

Самый грубый подход. Потребуем, чтобы:

$ norm(S)^k <= 1/e $

Вытащим отсюда $k$:

$ k ln norm(S) <= -1 $

Для сходящихся методов норма матрицы всегда меньше единицы. Мы это докажем.

$ k >= 1 / (-ln norm(S)) $

$ R := - ln norm(N) - "ассимптотическая скорость сходимости" $

Теперь рассмотрим основополагающую теорему о сходимости стационарных методов. 

#bbox[Критерий сходимости][
 Для того, чтобы стационарный метод сходился при любом начальном приближении, небходимо и достаточно, чтобы все собственные значения матрицы перехода были по модулю меньше единицы.

 $ abs((lambda(S))) < 1 $
]

Критерий плох тем, что надо постоянно искать собственные значения. Поэтому это теорема базовая.

Докажем необходимость. Пусть метод сходится при любом начальном приближении. Докажем, что все собственные значения по модулю меньше единицы. От противного. Пусть метод сходится, но существует хотя бы одно $abs(lambda(S)) >= 1$. Обозначим за $u$ собственный вектор, который соответствует этому собственному значению. $S u = lambda u$. В качестве начального приближения возьмём $x^((0)) := x^* + u$. Тогда $z^((0)) = u$. 

$ z^((k)) = S^k z^((0)) = S^k u = lambda^k u $

$ norm(z^((k))) = norm(lambda^k u) = abs(lambda)^k norm(u) $

$ u!=0, quad k->oo $

$ norm(z^((k))) arrow.not.r_(k->oo) 0 $

Получили противоречие.

Сформулируем лемму, которая понадобится для доказательства достаточности.

#bbbox[Лемма][
  Пусть все собственные значения матрицы $S$:

  $ abs(lambda_i (s)) < q, space i = 1, dots, n $

  Тогда существует такая невырожденная матрица $T$, что матрица $Lambda = T S T^(-1)$
  удовлетворяет такому условию: $norm(Lambda)_oo <= q$
]

Напоминание:
$ Lambda tilde S => "собственные значения у них совпадают" $

Теперь докажем достаточность.

$ abs(lambda (S)) < 1 $

$ q < 1 $

$ max_i abs(lambda_i (s)) < q $

Надо показать, что $lim_(k->oo) norm(z^((k))) = 0$.

Теперь привлекаем лемму. Существует такая невырожденная матрица $T$, что $norm(Lambda)_oo <= q $.

$ S = T^(-1) Lambda T $

Подставим $S$ в определение $z^((k))$.

$ S^k = T^(-1) Lambda^k T $

$ z^((k)) = S^k z^((0)) = T^(-1) Lambda^k T z^((0)) $

$ norm(z^((k)))_oo = norm(S^k z^((0)) = T^(-1) Lambda^k T z^((0)))_oo <= \
  <= norm(T^(-1))_oo norm(Lambda)_oo^k  norm(T)_oo norm(z^((0)))_oo <= \
  <= mu_oo(T) norm(Lambda^k)_oo norm(z^((0)))_oo <= mu_oo (T) q^k norm(z^((0)))_oo -->_(k->oo) 0
  $


#bbox[Теорема][
Для сходимости двуслойного стационарного метода при любом начальном приближении достаточно, чтобы хотя бы одна из норм матрицы перехода $S$, согласованная с какой-нибудь векторной, была меньше единицы.

$ norm(S) < 1 $
]

Собственное значение и вектор:

$ S u = lambda u $

$ abs(lambda) norm(u) = norm(lambda u) = norm(S u) <= norm(S) norm(u) $

$ abs(lambda) <= norm(S) $

Собственное значение всегда ограничено нормой.

$ norm(S) < 1 => abs(lambda) < 1 $


`29 Октября 2024`
== Метод простой итерации

Когда матрица $B = I$ 

Тогда матрица перехода:
$ S = I - tau A $

$ x^((k+1)) = x^((k)) - tau (A x^((k)) - f) $

Оптимизация скорости сходимости по параметру $tau$, то есть найдём такой $tau$ оптимальный.

Асимптотическая скорость сходимости $R = -ln norm(S)$.

Нужно максимизировать $R$, то есть, минимизировать $norm(S)$.

Предположим, что матрица $A$ симметрическая и положительно определённая.

$ A = A^* > 0 $

Все собственные значения вещественные и положительные. Обозначим минимальное и максимальное собственное значение за $alpha$ и $beta$:

$ 0 < alpha <= lambda_i (A) <= beta  $.

$ norm(S)_2 = max abs(lambda_i (S)) $

$ lambda_i (S) = lambda_i (I - tau A) = "свойства линейности" = 1 - tau lambda_i (A) $

$ norm(S)_2 = max_(1 <= i <= n) abs(1 - tau lambda_i (A)) $

Рассмотрим функцию на отрезке $[alpha, beta]$:

$ f(tau, lambda) = abs(1 - tau lambda) $

Принцип максимума (на веру):

$ max_(alpha <= lambda < beta) abs(1 - lambda tau) = max(f(tau, alpha), f(tau, beta)) $

Рассмотрим три случая (на самом деле один, остальные дома рассмотрим).

1. $1/beta <= tau <= 1/alpha$

$ 1 - tau alpha >= 1 - 1/ alpha alpha = 0 $
$ 1 - tau beta <= 1 - 1/ beta beta = 0 $

#figure(image("image.png", width: 40%))

Следующее очев, потому что слева $n$ значений, а справа отрезок $[alpha, beta]$:

$ max_(1 <= i <= n) abs(1 - tau lambda_i (A)) <= max_(alpha <= lambda <= beta) f(tau, lambda) $


$ max_(1 <= i <= n) abs(1 - tau lambda_i (A)) <= max_(alpha <= lambda <= beta) f(tau, lambda) = max(f(tau, alpha), f(tau, beta)) $

Следующее очевидно по аналогичным причинам: слева $n$ значений, справа --- два:

$ max_(1 <= i <= n) abs(1 - tau lambda_i (A)) >=
  max(abs(1 - tau alpha), abs(1 - tau beta))
$

$
max_(1 <= i <= n) abs(1 - tau lambda_i (A)) >=
  max(abs(1 - tau alpha), abs(1 - tau beta))
  = max(f(tau, alpha), f(tau, beta))
$

Получаем, что:

$ max_(1<=i<=n) abs(1 - tau lambda_i (A)) = norm(S)_2 = 
  max(f(tau, alpha), f(tau, beta))
$

Вот оно оптимальное значение:

$ tau_0 = 2 / (alpha + beta) $

Проведём доказательство следующим образом:

$ 1 - tau_0 alpha = 1 - 2/(alpha+beta) alpha = (beta - alpha)/(alpha + beta) > 0 $
$ 1 - tau_0 beta = 1 - 2/(alpha+beta)beta = (alpha - beta)/(alpha+beta) < 0 $

$ f(tau_0, alpha) = (beta - alpha) / (beta + alpha) $

$ f(tau_0, beta) = (beta - alpha) / (beta + alpha) $

$ norm(S)_2 = (beta - alpha) / (beta + alpha) < 1 $

Рассмотрим первый случай $tau < tau_0$:

$ 1 - tau alpha > 1 - tau_0 alpha = (beta - alpha)/(beta+alpha) $

$ f(tau, alpha) = abs(1 - tau alpha) > (beta - alpha)/(beta + alpha) $

$ max(f(tau, alpha), f(tau, beta)) >= f(tau, alpha) > (beta - alpha)/(beta + alpha) $

Получили, что $norm(S)_2 > (beta - alpha)/(beta + alpha)$

Аналогично во втором случае $tau > tau_0$.

$ R = - ln norm(S)_2 = - ln (beta - alpha)/(beta + alpha) $

К чему можно преобразовать эту формулу асимптотической скорости сходимости? Свяжем её с определением качества сходимости, которое мы назвали числом обусловленности.

$ mu_2 (A) = norm(A)_2 norm(A^(-1))_2 = max_i lambda_i (A) dot max_i lambda_i (A^(-1)) = \
  = max_i lambda_i (A) dot max_i 1/(lambda_i) (A)
  = (max_i lambda_i (A)) / (min_i lambda_i (A))
  = beta / alpha
$

$ R = - ln (beta - alpha)/(beta+ alpha) = ln (beta+alpha)/(beta-alpha) = ln (beta/alpha+1)/(beta/alpha-1) = ln (mu_2 (A) + 1)/(mu_2 (A) - 1) $

== Методы Якоби (якобы Метод)

Тут матрица $B$ будет диагональной.

$ A := A_l + D + A_v  $

$A_l$ --- нижнетреугольная, $A_v$ --- верхнетреугольная, $D = "diag"(a_11, ... , a_(n n))$.

Пусть все диагональные элементы отличны от нуля $alpha_(i i) != 0$.

$ B := D, space tau := 1 $

Подставим в каноническую форму записи:

$ D(x^((k+1))-x^((k))) + (A_l + D + A_v)x^((k)) = f $

$ D x^((k+1)) - D x^((k)) + A_l x^((k)) + D x^((k)) +A_v x^((k)) = f $

$ D x^((k+1)) = f - (A_l + A_v) x^((k)) $

Эту матричную запись легко переписать в скалярный за счёт диагональной матрицы.

$ a_(i i) x_i^((k+1)) = f_i - sum_(j=1, j!=i)^n a_(i j) x_j^((k)) $

$ x_i^((k+1)) = (f_i - sum_(j=1, j!=i)^n a_(i j) x_j^((k)))/a_(i i) $

Потрясём матрицу $S$:

$ S = I - tau B^(-1) A = I - D^(-1)(A_l + D + A_v) = -D^(-1) (A_l + A_v) $

Для того, чтобы метод Якобы сходился, необходимо и достаточно, чтобы собственные значения этой матрицы были меньше единички по модулю.

Но нас такое не вдохновляет, мы хотим по виду $A$ сразу понять.

#def[
  Строгое диагональное преобладание или строгое условие Адамара, это когда $forall i$:

  $ abs(a_(i i)) > sum_(j=1\ j!=i) abs(a_(i j)) $
]

#bbbox[Теорема Адамара][
  Матрица со строгим диагональным преобладанием невырождена.
][
  Пусть матрица имеет строгое диагональное обладание и вырождена (противное).
  Тогда $A x = 0$ имеет ненулевое решение. Выделим максимальную по модулю компоненту этого решения, $abs(x_k) = max_i abs(x_i) > 0$. Поработаем с ним:

  $ sum_(j = 1)^n a_(n j) x_j = 0 $

  $ a_(k k) x_k + sum_(j=1 \ j!= n)^n a_(k j) x_k = 0 $

  $ a_(k k) x_k = - sum_(j=1\ j!=k)^n a_(k j) x_j $

  $ abs(a_(k k)) abs(x_k) <= sum_(j=1)^n abs(a_(k j)) abs(x_j) <= abs(x_k) sum_(j=1)^n abs(a_(k j)) = \
  = abs(k k) <= sum_(j=1 \ j!= k)^n abs(a_(k j))
  $
]

#bbbox[Следствие из теоремы Адамара][
  У вырожденной матрица нарушено ходя бы одно строгое условие Адамара.
]

#bbbox[Теорема Гершгорина][
  Каждое собственное значение матрицы $A$ принадлежит по крайней мере одному из кругов Гершгорина этой матрицы.

  $ abs(lambda - a_(i i)) <= sum_(j=1\ j!=i)^n abs(a_(i j)), space i = 1, dots, n $
][
  Пусть $lambda$ --- собственное значение.

  $ det(A - lambda I) = 0 $

  $ abs(a_(k k) - lambda) <= sum_(j=1\ j!=n)^n abs(a_(k j)) $
]


  Если матрица имеет строгое диагональное преобладание, то матрица Якоби сходится при любом начальном приближении.

$ S = -D^(-1) (A_l + A_v) $

$ S_(i i) = 0 $

$ S_(i j) = -a_(i j)/(a_(i i)) $

$ abs(lambda_i (S) - 0) <= sum_(j=1\ j!=i)^n abs(-a_(i j)/a_(i i)) = sum_(j=1\ j!=i)^n abs(a_(i j)/a_(i i)) < 1 $

Тогда по критерию этот метод сходится. $abs(lambda_i (S)) < 1$.

`5 Ноября 2024`

== Метод Зейделя

$ B = A_l + D, quad tau = 1 $

Подставляем в каноническую форму:

$ (A_l + D) (x^((k+1)) - x^((k))) + (A_l + D + A_v) x^((k)) = f $

$ (A_l + D) x^((k+1)) = -A_v x^((k)) + f $

Запишем в скалярном виде:

$ sum_(j=1)^(i-1) a_(i j) x_j^((k+1)) + a_(i i)x_i^((k+1)) = -sum_(j=i+1)^n a_(i j) x_j^((k)) + f_i $

$ x_i^((k+1)) = (f_i - sum_(j=1)^(i-1) a_(i j)x_j^((k+1)) - sum_(j=i+1)^n a_(i j) x_j^((k)))/a_(i i) quad (1) $ 

Вот так вот крутимся-крутимся.

Вопрос со сходимостью. Сейчас будем работать с матрицей $S$:

$ S = I - tau B^(-1) A = I - (A_l + D)^(-1) (A_l + D + A_v) = -(A_l + D)^(-1) A_v
$

Для того, чтобы сходился метод, необходимо и достаточно, чтобы собственные числа этой матрицы были по модулю меньше единицы.

$ det(-(A_l + D)^(-1) A_v - lambda I) = 0 $

$ det(-(A_l + D)^(-1) (A_v + lambda (A_l + D))) = 0 $

$ det(-(A_l + D)^(-1)) dot  det(A_v + lambda (A_l + D))) = 0 $

$ "т.к. " det(-(A_l + D)^(-1)) != 0 $

$ det(A_v + lambda(A_l + D)) = 0 $

$ abs(lambda) < 1 $

#bbbox[Теорема][
  Если в матрице $A$ диагональное преобладание, то метод Зейделя сходится при любом начальном приближении.
]

Это очень мощная, хорошая теорема. Сейчас мы её докажем.

Пусть $lambda$ произвольный корень уравнения: 

$ det(A_v + lambda(A_l + D)) = 0 $

У вырожденной матрицы нарушено хотя бы одно строгое условие Адамара.

Вот матрица:

$ A_v + lambda(A_l + D) $

Выпишем $k-$ ю строчку, в которой нарушено диагональное преобладание:

$ abs(lambda a_(k k)) <= sum_(j=1)^(k-1) abs(lambda a_(k j)) + sum_(j=k+1)^n abs(a_(k j)) $

$ abs(lambda) (abs(a_(k k)) - sum_(j=1)^(k-1) abs(a_(k j))) <= sum_(j=k+1)^n abs(a_(k j)) $

$ abs(lambda) <= (sum_(j=k+1)^n abs(a_(k j))) / (abs(a_(k k)) - sum_(j=1)^(k-1) abs(a_(k j)))   $

$ abs(a_(k k)) > sum_(j=1)^(k-1) abs(a_(k j)) + sum_(j=k+1)^(n) abs(a_(k j)) $

$ sum_(j=k+1)^(n) abs(a_(k j)) < abs(a_(k k)) - sum_(j=1)^(k-1) abs(a_(k j)) $

$ abs(lambda) < 1 $

Вот это доказателство вот этой теоремы.

Есть ещё одна теорема о сходимости, но мы её доказывать не будем, только посмотрим. Давайте сформулируем.

#bbbox[Теорема][
  Если матрица системы $A$ симметрическая и положительно определённая, то метод Зейделя сходится при любом начальнмо приближении.
]



Метод Зейделя --- это частный случай метода релаксации, но это мы попозже разберём.

Перейдём к следующему методу, но сначала проделаем некоторую работу. Давайте под другим углом посмотрим на этот метод, посмотрим переход:

$ y = vec(x_1^((k+1)), dots.v, x_(i-1)^((k+1)), x_i^((k)), x_(i+1)^((k)), dots.v, x_n^((k))) quad 
  z =  vec(x_1^((k+1)), dots.v, x_(i-1)^((k+1)), x_i^((k+1)), x_(i+1)^((k)), dots.v, x_n^((k)))
$

$ t = A y - f wide r = A z - f $

$ t_i = sum_(j=1)^(i-1) a_(i j) x_j^((k+1)) + a_(i i) x_i^((k)) + sum_(j=i+1)^n a_(i j) x_j ^((k)) - f_i $

$ r_i = sum_(j=1)^(i-1) a_(i j) x_j((k+1)) + a_(i i) x_i^((k + 1)) + sum_(j=i+1)^n a_(i j) x_j ^((k)) - f_i$

$ r_i equiv 0 "соответсвтует методу Зейделя" $

То есть обнуление невязки это и есть метод. Очев, подставьте в (1), чтоб было очев. 

Вычтем из одного уравнения другое:

$ t_i - r_i = a_(i i) (x_i^((k))-x_i^((k+1))) := a_(i i) alpha $

$ r_i = t_i - alpha a_(i i) equiv 0 $

Метод Зейделя --- это метод полной релаксации.

== Метод частичной релаксации

Тут мы хотим, чтобы $r$ был не ноль, но поменьше $t$.

$ abs(t_i - alpha a_(i i)) < abs(t_i) $

$ abs(alpha - t_i / a_(i i)) < abs(t_i / a_(i i)) $

1) $ t_i/a_(i i) > 0 wide abs(alpha - t_i / a_(i i)) < t_i / a_(i i) $
   $ -t_i/a_(i i) < alpha - t_i/a_(i i) < t_i / a_(i i) wide
    0 <alpha < 2 t_i / a_(i i)
   $

2) $ t_i/a_(i i) < 0 wide  2 t_i/a_(i i) < alpha < 0 $

\

$ alpha = omega t_i / a_(i i) wide omega in (0, 2) $

Тогда будет уменьшение компоненты невязки.

$ abs(r_i) < abs(t_i) $

$ r_i = t_i - alpha a_(i i) = t_i  - omega t_i/a_(i i) a_(i i) = (1 -omega)t_i $

$ r_i = (1-omega)t_i $

\

$ x_i^((k+1)) = (omega f_i - omega sum_(j=1)^(i-1) a_(i j)x_j^((k)) + (1-omega) a_(i i) x_j^((k)) - omega sum_(j=i+1)^n a_(i j)x_j^((k))) / a_(i i)  $

Теперь восстановим каноническую форму записи, ведь мы не знаем ни $tau$, ни $B$. В общем, надо из скалярной формы записи получить матричную.

$ S = I - tau B^(-1) A $

Переработаем формулу для $x_i^((k+1))$. К этой пижне добавим и вычтем $sum_(j=1)^(i-1) a_(i j) x_j^((k)) omega $.
Это дома проделать обязательно, на лекции нет времени:

$ a_(i i) (x_i^((k+1))- x_i^((k))) + omega sum_(j=1)^(i-1) a_(i j)(x_j^((k+1)) - x_j ^((k))) + omega sum_(j=1)^(n) a_(i j) x_j^((i)) = omega f_i $

Нужно будет заполучить:

$ B (x^((k+1)) - x^((k))) / tau + A x^((k)) = f $

// $ a_(i i) (x_i^((k+1)) - x_i^((k))) + omega sum_(j=1)^(i-1) a_(i j) (x_j^((k+1)) - x_j^((k))) + omega sum_(j=1)^n a_(i j) x_j^((n)) = omega f_i $

Переходим теперь к матричной форме записи:

$ D(x^((k+1)) - x^((k))) + omega A_l (x^((k+1)) - x^((k))) + omega A x^((k)) = omega f $

Возьмём:

$ &tau:=omega\
  & B := D + omega A_l
$

$ (D + omega A_l) (x^((k+1)) - x^((k)))/omega  + A x^((k)) = f $

Вот она --- каноническая формула записи для этого метода. А $omega=1$ --- это метод Зейделя.

$ S = I - tau B^((-1)) A = I - omega(D + omega A_l)^(-1) A $

Первый шаг, который здесь делается, это убрать эту некрасивую обратную матрицу.

$ det(I - omega(D + omega A_l)^(-1) - lambda I) = 0 $

$ det((D + omega A_l)^(-1)) dot det((1-lambda)(D+omega A_l) - omega (A_l + D + A_v)) = 0 $

$ det((1-lambda)(D+omega A_l) - omega(A_l + D + A_v)) = 0 $

$ det((1-omega)D - lambda(D+omega A_l) - omega A_v) = 0 $

$ abs(lambda) < 1 $

Оказывается теорема с достаточными условиями сходимости (симметрическая + положительно определённая) работает для всех релаксационных методов.

Теперь мы идём в комплексную плоскость. Рассмотрим конформное отображение круга в следующую пижню:

$ |lambda| < 1 <=> lambda = (mu+1)/(mu-1), "Re" mu<0 $

Для сходимости достаточно, чтобы все реальные части $mu$ были отрицательны.

`12 Ноября 2024`
== Многочлены Чебышёва

#def[
  $ T_n (x), space n>= 0 $
  $ T_0 (x) = 1 quad T_1 (x) = x $
  $ T_(n+1) (x) = 2 x T_n (x) - T_(n-1) (x) $
]

$ T_2 (x)=2x^2 - 1 $
$ T_3 (x) = 4x^3 - 3x $

Вспомним тригонометрическую формулу:

$ cos((n+1)theta ) = 2 cos theta cos n theta - cos((n-1)theta) $

$ theta := arccos x $

$ T_n (x) = cos(n arccos x) $

$ cos((n+1)theta) = T_(n+1) (x) $

$ 2 cos theta cos n theta = 2x cos(n arccos x) = 2 x T_n (x) $

$ cos ((n-1)theta) = T_(n-1)(x) $

Ограничение, $abs(x) <=1$.

Рекуррентная запись полинома Чебышёва соответствует характеристическому уравнению:

$ mu^2  - 2 x mu + 1 = 0 $

$ mu_(1,2) = x plus.minus  sqrt(x^2 - 1) $

$ T_n ( x) = C_1 mu_1^n + C_2 mu_2^n - "на веру, доказывать не будем" $



$ T_n ( x) = ((x+sqrt(x^2-1))^n + (x-sqrt(x^2-1))^n)/2 $

Нули полинома:

$ T_n (x) = cos(n arccos x) = 0 $

$ n arccos x = pi/2 + m pi = (pi (2m + 1))/2 $

$ x_m = cos (pi(2m + 1))/(2n) where m = 0,1,dots,n-1 $

$ abs(T_n (x)) <= 1 $

Точки экстремума:

$ x_((m)) = cos (pi m)/n where m=0,dots, n $

Если построить немного другой полином (убирает коэффициент при старшем члене):

$ overline(T)_n (x) = 2^(1-n) T_n (x) = x^n + dots $

Полиномы называются _наименее уклоняющиеся от нуля_.

#bbox[Лемма][
  Если $P_n (x)$ это многочлен степени $n$ со старшим коэффициентом 1, то справебыдло следующее:

  $ max_([-1, 1]) abs(P_n (x)) >= max_([-1, 1]) abs(overline(T)_n (x)) = 2^(1-n) $
]

Преобразование:
$ x' := (b+a)/2 + (b-a)/2 x $

Перегоняет отрезок:

$ [-1, 1] --> [a, b] $

Переформулируем на отрезок $[a, b]$:

$ overline(T)_n ((2x-(b+a))/(b-a))  $

$ max_[a, b] abs(P_n (x)) >= max_[a, b] abs(overline(T)_n^[a,b] (x)) = (b-a)^n 2^(1-2n) $

$ x_m = (b+a)/2 + (b-a)/2 cos pi(2m+1)/(2n) $

Теперь можем переходить к методу Ричардсона.

== Метод Ричардсона

Нестанционарный, но напоминает метод простой итерации.

$ B = I $

$ (x^((k+1)) - x^((k)))/tau_(k+1) + A x^((k)) = f $

$ x^((k+1)) = x^((k)) - tau_(k+1) A x^((k)) + tau_(k+1) f $

По аналогии с методом простой итерации мы получим:

$ z^((k+1)) = S_(k+1) z^((k)) $

Здесь матрица перехода на каждом шаге своя, так как метод нестационарный.

$ S_(k+1) = I - tau_(k+1) A $

$ z^((k)) = S_k z^((k-1)) = S_k S_(k-1) z^((k-1)) = S_k S_(k-2) dots S_1 z^((0)) := T_k z^((0))  $

Это не Полином Чебышёва! Это _разрешающая матрица $k$-го шага_. 

$ z^((k)) = T_k z^((0)) $

$ z^((k)) = S^k z^((0)) $

// $ norm(S)_2 tau - "минимизировали" $

// $ norm(S^k)_2  $

Будем минимизировать резрешающую матрицу.

$ A := A^* > 0, quad 0 < alpha <= lambda_i (A) <= beta $

$ T_k = (I - tau_k A) (I - tau_(k-1) A) dots (I - tau_1 A) $

$ T_k - "симметрическая" $

$ T_k^* = (I - tau_1 A)^* (I - tau_2 A)^* dots (I - tau_k A)^* = (I - tau_1 A)(I - tau_2 A) dots (I-tau_k A) $

Любые два полинома от матрицы перестановочны: $P(A)Q(A) = Q(A)P(A)
$

$ norm(T_k)_2 = max abs(lambda_i (T_k)) $


$ T_k = (I - tau_k A) (I - tau_(k-1) A) dots (I - tau_1 A) $


$ A x^((i)) = lambda_i (A)  x^((i)), quad x^((i)) != 0 $

$ (I - tau_(j) A) x^((i)) = (1 - tau_j lambda_i (A) ) x^((i)) $

Теперь проводим цепочку:

$ T_k x^((i)) = (I - tau_k A)(I - tau_(k-1)A)dots(I-tau_2 A)(I - tau_1 A) x^((i)) $

$ (I - tau_1 A) x^((i)) = (1 - tau_1 lambda_i)x^((i)) - "число" $

$ T_k x^((i)) = (1-tau_1 lambda_i) (1-tau_2 lambda_i)... = product_(j=1)^k (1-tau_j lambda_i) x^((i)) $

$  norm(T_k)_2 = max_(1 <= i <= n) abs( product_(j=1)^k (1-tau_j lambda_i (A))) $

$ norm(T_k)_2 <= max_[alpha, beta] abs(product_(j=1)^k (1-tau_j mu)) $

Введём полином и опишем его свойства:

$ P_k (mu) := product_(j=1)^k (1-tau_j mu) $

$ P_k (0) = 1 $

Надо подобрать такой полином по набору параметров $tau_1, dots, tau_n$, чтобы он наименее отклонялся от нуля.

Оказывается любой полином со свойством $P_k (0) = 1$ представим в виде:

$ P_k (mu) := product_(j=1)^k (1-tau_j mu) $

Любой полином раскладывается:

$ Q_k = a(mu-mu_1)(mu-mu_2)dots(mu-mu_k) $

$ Q_k(0) = 1 - "сохраняем условие" $

$ Q_k = a product_(j=1)^k  (mu-mu_j) $

$ Q_k (0) = a product_(j=1)^k (-mu_j) = (-1)^k) a product_(j=1)^k mu_j = 1 $

$ a = (-1)^k/(product_(j=1)^k mu_j) $

$ Q_k (mu) = (-1)^k/(product_(j=1)^k mu_j) product_(j=1)^k (mu - mu_j)  = 
(-1)^k product_(j=1)^k (mu-mu_j)/mu_j 
 = product_(j=1)^k (mu_j - mu)/mu_j  $

 $ tau_j = 1/ mu_j $



#bbbox[Лемма][
 
Среди всех многочленов с вещественными корнями и удовлетворяющих условию $P_k (0)=1$, наименее отклоняется от нуля:

$ P_k^0 (mu) = (T_k ( (2mu - (beta+alpha))/(beta-alpha) ))/ t_k $

Здесь уже $T_k$ это полином Чебышёва.

$ t_k = T_k (- (alpha + beta)/(beta-alpha)) $

Ещё в этой лемме утверждается, что:

$ max_(alpha <= mu <= beta) abs( P_k^0 (mu) ) = 1/abs(t_k) $]